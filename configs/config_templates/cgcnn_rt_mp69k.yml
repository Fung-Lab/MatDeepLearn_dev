trainer: property

task:
  # run_mode: train
  identifier: "rt_mp69k"
  reprocess: False
  run_id: ""
  parallel: True
  device: "cuda:0"
  seed: 0
  # seed=0 means random initalization
  write_output: True
  parallel: True
  # Training print out frequency (print per n number of epochs)
  verbosity: 1
  wandb:
    use_wandb: True
    wandb_entity: "fung-lab"
    wandb_project: "rt"
    notes: "try MP dataset"
    tags: ["test"]
    track_params:
      - "optim.lr"
      - "optim.batch_size"
      - "optim.max_epochs"
      - "dataset.preprocess_params.num_offsets"
      - "dataset.preprocess_params.edge_calc_method"
      - "dataset.preprocess_params.all_neighbors"
      - "dataset.preprocess_params.edge_steps"
      - "dataset.preprocess_params.use_degree"
    log_artifacts:
      - "/Users/sidbaskaran/Desktop/research/MatDeepLearn_dev/matdeeplearn/models/cgcnn_vn.py"
      # - "/global/cfs/projectdirs/m3641/Sidharth/MatDeepLearn_dev/matdeeplearn/models/cgcnn_vn.py"
    metadata:
      architecture: "RTModel"
      cluster: "fung-cluster"
      dataset: "MP_data_69K"
    sweep:
      parallel: True
      do_sweep: False # ignore rest of config if False
      system: "phoenix_slurm" # one of "local", "phoenix_slurm"
      job_config: "/nethome/sbaskaran31/projects/Sidharth/MatDeepLearn_dev/configs/jobs/phoenix_slurm.yml"
      count: 3
      sweep_file: "/nethome/sbaskaran31/projects/Sidharth/MatDeepLearn_dev/configs/sweeps/cgcnn_vn_sweep_d1.yml"

model:
  name: RTModel
  load_model: False
  save_model: True
  model_path: "rt.pth"
  # model hyperparams
  hyperparams:
    # node_dim: 100
    # edge_dim: 25
    # output_dim: 1
    node_hidden: 128
    edge_hidden_1: 128
    edge_hidden_2: 128
    heads: 4
    layers: 4
    dropout: 0.0
    disable_edge_updates: False
    node_level_output: False
    edge_level_output: False

optim:
  max_epochs: 250
  lr: 0.002
  loss:
    loss_type: "TorchLossWrapper"
    loss_args: {"loss_fn": "l1_loss"}
  batch_size: 16
  optimizer:
    optimizer_type: "AdamW"
    optimizer_args: {}
  scheduler:
    scheduler_type: "ReduceLROnPlateau"
    scheduler_args: {"mode":"min", "factor":0.8, "patience":10, "min_lr":0.00001, "threshold":0.0002}

dataset:
  processed: False # if False, need to preprocessor data and generate .pt file
  force_preprocess: False
  num_examples: 0 # set to 0 when using full dataset, else will take the first "num_examples" examples
  # Path to data files
  src: "/Users/sidbaskaran/Desktop/research/MP_data_69K/raw"
  # src: "/global/cfs/projectdirs/m3641/Sidharth/datasets/MP_data_69K/raw"
  target_path: "/Users/sidbaskaran/Desktop/research/MP_data_69K/targets.csv"
  # target_path: "/global/cfs/projectdirs/m3641/Sidharth/datasets/MP_data_69K/targets.csv"
  pt_path: "/Users/sidbaskaran/Desktop/research/MP_data_69K/raw/processed/ocp"
  # pt_path: "/global/cfs/projectdirs/m3641/Sidharth/datasets/MP_data_69k/raw/processed/ocp"
  # transforms
  transforms:
    - name: GetY
      args:
        index: 0
      otf: False
  # use for passing into global config
  # one of MDL, ASE, OCP
  use_sweep_params: False
  apply_pre_transform_processing: True
  # use again for passing into global config
  data_format: "json"
  node_representation: "onehot"
  additional_attributes: []
  # Print out processing info
  verbose: True
  # graph specific settings: preprocessing hyperparams
  preprocess_params:
    cutoff_radius : 5.0
    n_neighbors : 250
    process_batch_size : 25
    edge_calc_method: "ocp"
    num_offsets: 1
    edge_steps : 25
    all_neighbors: True
    use_degree: False
  # Ratios for train/val/test split out of a total of 1
  train_ratio: 0.8
  val_ratio: 0.05
  test_ratio: 0.15
