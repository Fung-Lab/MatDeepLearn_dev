trainer: property

task:
  # run_mode: train
  identifier: "cgcnn_vn_2dnpj"
  reprocess: False
  run_id: ""
  parallel: False
  device: "cuda:1"
  seed: 0
  # seed=0 means random initalization
  write_output: True
  parallel: False
  # Training print out frequency (print per n number of epochs)
  verbosity: 1
  wandb:
    use_wandb: True
    wandb_entity: "fung-lab"
    wandb_project: "cgcnn_vn_new"
    notes: "try 2d dataset"
    tags: ["test"]
    track_params:
      - "model.hyperparams.mp_pattern"
      - "model.hyperparams.pool"
      - "model.hyperparams.virtual_pool"
      - "model.hyperparams.gc_count"
      - "optim.lr"
      - "optim.batch_size"
      - "optim.max_epochs"
      - "dataset.preprocess_params.num_offsets"
      - "dataset.preprocess_params.edge_calc_method"
      - "dataset.preprocess_params.all_neighbors"
      - "dataset.preprocess_params.edge_steps"
      - "dataset.preprocess_params.use_degree"
    log_artifacts:
      # - "/Users/sidbaskaran/Desktop/research/MatDeepLearn_dev/matdeeplearn/models/tokengt/tokengt_graph_encoder.py.py"
      # - "/storage/home/hcoda1/9/sbaskaran31/p-vfung3-0/MatDeepLearn_dev/matdeeplearn/models/cgcnn_vn.py"
      # - "/global/cfs/projectdirs/m3641/Sidharth/MatDeepLearn_dev/matdeeplearn/models/cgcnn_vn.py"
      - "/nethome/sbaskaran31/projects/Sidharth/MatDeepLearn_dev/matdeeplearn/models/tokengt/tokengt_graph_encoder.py"
    metadata:
      architecture: "TokenGT"
      cluster: "local"
      dataset: "2D_data_npj"
    sweep:
      parallel: True
      do_sweep: False # ignore rest of config if False
      system: "phoenix_slurm" # one of "local", "phoenix_slurm"
      job_config: "/nethome/sbaskaran31/projects/Sidharth/MatDeepLearn_dev/configs/jobs/phoenix_slurm.yml"
      count: 3
      sweep_file: "/nethome/sbaskaran31/projects/Sidharth/MatDeepLearn_dev/configs/sweeps/cgcnn_vn_sweep_d1.yml"

model:
  name: TokenGT
  load_model: False
  save_model: True
  model_path: "tokengt.pth"
  # model hyperparams
  hyperparams:
    num_atoms: 51200
    num_edges: 51200
    rand_node_id: False
    rand_node_id_dim: 64
    orf_node_id: False
    orf_node_id_dim: 64
    lap_node_id: True
    lap_node_id_k: 8
    lap_node_id_sign_flip: False
    lap_node_id_eig_dropout: 0.0
    type_id: True
    stochastic_depth: False
    performer: False
    performer_finetune: False
    performer_feature_redraw_interval: 1000
    performer_generalized_attention: False
    performer_auto_check_redraw: True
    num_encoder_layers: 12
    embedding_dim: 768
    ffn_embedding_dim: 768
    num_attention_heads: 32
    dropout: 0.1
    attention_dropout: 0.1
    activation_dropout: 0.1
    layerdrop: 0.0
    encoder_normalize_before: False
    layernorm_style: "postnorm"
    apply_graphormer_init: False
    activation_fn: "gelu"
    freeze_embeddings: False
    n_trans_layers_to_freeze: 0
    export: False
    traceable: False
    q_noise: 0.0
    qn_block_size: 8
    return_attention: False

optim:
  max_epochs: 250
  lr: 0.002
  verbosity: 1
  loss:
    loss_type: "TorchLossWrapper"
    loss_args: {"loss_fn": "l1_loss"}
  batch_size: 2
  optimizer:
    optimizer_type: "AdamW"
    optimizer_args: {}
  scheduler:
    scheduler_type: "ReduceLROnPlateau"
    scheduler_args: {"mode":"min", "factor":0.8, "patience":10, "min_lr":0.00001, "threshold":0.0002}

dataset:
  processed: False # if False, need to preprocessor data and generate .pt file
  force_preprocess: False
  num_examples: 0 # set to 0 when using full dataset, else will take the first "num_examples" examples
  # Path to data files
  src: "/nethome/sbaskaran31/projects/Sidharth/2D_data_npj/raw"
  # src: "/Users/sidbaskaran/Desktop/research/MP_data_69K/raw"
  # src: "/storage/home/hcoda1/9/sbaskaran31/p-vfung3-0/MP_data_69K/raw"
  # src: "/global/cfs/projectdirs/m3641/Sidharth/datasets/MP_data_69K/raw"
  target_path: "/nethome/sbaskaran31/projects/Sidharth/2D_data_npj/targets.csv"
  # target_path: "/Users/sidbaskaran/Desktop/research/MP_data_69K/targets.csv"
  # target_path: "/storage/home/hcoda1/9/sbaskaran31/p-vfung3-0/MP_data_69K/targets.csv"
  # target_path: "/global/cfs/projectdirs/m3641/Sidharth/datasets/MP_data_69K/targets.csv"
  pt_path: "/nethome/sbaskaran31/projects/Sidharth/2D_data_npj/processed/ocp"
  # pt_path: "/Users/sidbaskaran/Desktop/research/MP_data_69K/raw/processed/ocp"
  # pt_path: "/storage/home/hcoda1/9/sbaskaran31/p-vfung3-0/MP_data_69K/raw/processed/ocp"
  # pt_path: "/global/cfs/projectdirs/m3641/Sidharth/datasets/MP_data_69k/raw/processed/ocp"
  # transforms
  transforms:
    - name: GetY
      args:
        index: 0
      otf: False
    # - name: VirtualNodeGeneration
    #   args:
    #     virtual_box_increment: 3
    #   otf: False
    # - name: VirtualEdgeGeneration
    #   args:
    #     attrs: ["rr", "rv"]
    #     rr_cutoff: 12.0
    #     rv_cutoff: 12.0
    #   otf: False
    #   batch: True
    - name: TokenGTGeneration
      args:
        placeholder: "test"
      otf: False
      batch: False
  # use for passing into global config
  # one of MDL, ASE, OCP
  use_sweep_params: False
  apply_pre_transform_processing: True
  # use again for passing into global config
  data_format: "json"
  node_representation: "onehot"
  additional_attributes: []
  # Print out processing info
  verbose: True
  # graph specific settings: preprocessing hyperparams
  preprocess_params:
    cutoff_radius : 8.0
    n_neighbors : 250
    process_batch_size : 100
    edge_calc_method: "ocp"
    num_offsets: 1
    edge_steps : 100
    all_neighbors: True
    use_degree: False
    max_node: 512
    max_edge: 2048
  # Ratios for train/val/test split out of a total of 1
  train_ratio: 0.8
  val_ratio: 0.05
  test_ratio: 0.15
