trainer: property

task:
  # run_mode: train
  identifier: "torchmd_net_vn-all-neighbor-OCP"
  reprocess: False
  parallel: True
  resume_run: False
  gpu: "cuda:0"
  seed: 0
  # seed: 0 means random initalization
  write_output: True
  parallel: True
  # Training print out frequency (print per n number of epochs)
  verbosity: 1
  wandb:
    use_wandb: True
    wandb_entity: "fung-lab"
    wandb_project: "torchmd_net_vn"
    notes: "TorchMD net modified to support custom pooling routines and virtual nodes"
    tags: ["test"]
    log_artifacts:
      - path: "/nethome/sbaskaran31/projects/Sidharth/MatDeepLearn_dev/configs/examples/torchmd_net_vn/config_torchmd_net_vn.yml"
        name: "train config"
        type: "config"
      - path: "/nethome/sbaskaran31/projects/Sidharth/MatDeepLearn_dev/matdeeplearn/models/torchmd_net_vn.py"
        name: "torchmd_net_vn model"
        type: "model"
    metadata:
      architecture: "torchmd_net_vn"
      cluster: "fung-cluster"
      dataset: "hMOF-5K"
    sweep:
      do_sweep: False
      sweep_file: "/nethome/sbaskaran31/projects/Sidharth/MatDeepLearn_dev/configs/cgcnn_vn_sweep.yml"

model:
  name: torchmd_net_vn
  load_model: False
  save_model: True
  model_path: "torchmd_net_vn.pth"
  # model hyperparams
  hyperparams:
    hidden_channels: 128
    num_layers: 6
    num_rbf: 50
    rbf_type: "expnorm"
    trainable_rbf: True
    activation: "silu"
    attn_activation: "silu"
    neighbor_embedding: True
    num_heads: 8
    distance_influence: "both"
    cutoff_lower: 0.0
    cutoff_upper: 5.0
    max_z: 100
    max_num_neighbors: 250
    num_post_layers: 1
    post_hidden_channels: 64
    pool: "global_mean_pool" # pooling reduction scheme
    virtual_pool: "AtomicNumberPooling" # pooling method
    pool_choice: "virtual" # whether to use virtual or real nodes in RealVirtualPooling
    mp_pattern: ["rr", "rv"]
    aggr: "add"

optim:
  max_epochs: 100
  lr: 0.002
  loss:
    loss_type: "TorchLossWrapper"
    loss_args: {"loss_fn": "l1_loss"}
  batch_size: 50
  optimizer:
    optimizer_type: "AdamW"
    optimizer_args: {}
  scheduler:
    scheduler_type: "ReduceLROnPlateau"
    scheduler_args: {"mode":"min", "factor":0.8, "patience":10, "min_lr":0.00001, "threshold":0.0002}

dataset:
  processed: True # if False, need to preprocessor data and generate .pt file
  # Path to data files
  src: "/nethome/sbaskaran31/projects/Sidharth/hMOF/raw_5k/data.json"
  target_path: ""
  pt_path: "/nethome/sbaskaran31/projects/Sidharth/hMOF/raw_5k/ocp-torchmd"
  # transforms
  transforms:
    - name: GetY
      args:
        index: 5 # methane adsorption uptake
      otf: False
    - name: VirtualNodes
      args:
        virtual_box_increment: 3
        attrs: ["rr", "rv"]
        rr_cutoff: 5.0
        rv_cutoff: 5.0
      otf: False
  # use for passing into global config
  # one of MDL, ASE, OCP
  use_sweep_params: False
  # use again for passing into global config
  data_format: "json"
  node_representation: "onehot"
  additional_attributes: []
  # Print out processing info
  verbose: True
  # graph specific settings: preprocessing hyperparams
  preprocess_params:
    cutoff_radius : 5.0
    n_neighbors : 250
    edge_calc_method: "ocp"
    num_offsets: 1
    edge_steps : 50
    all_neighbors: True
    use_degree: False
  # Ratios for train/val/test split out of a total of 1
  train_ratio: 0.8
  val_ratio: 0.05
  test_ratio: 0.15
